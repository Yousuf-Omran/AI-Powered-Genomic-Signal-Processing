{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Import all required libraries.**"
      ],
      "metadata": {
        "id": "5jBManZt4d4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pandas import read_excel\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.fftpack import fft\n",
        "from keras import Input\n",
        "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Dropout, BatchNormalization, Activation\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.models import Model, clone_model, load_model\n",
        "from keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, roc_curve, roc_auc_score, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "aoVaT2tftVNU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read data from excel.**"
      ],
      "metadata": {
        "id": "lyCCr0Gw4onb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fC9j926VjU7m"
      },
      "outputs": [],
      "source": [
        "# numeric representation function\n",
        "def EIIP(data_, n):\n",
        "    row=len(data_)\n",
        "    data= np.zeros([row,n])\n",
        "    for i in range(row):\n",
        "        for j in range(n):\n",
        "            char= data_[i][0][j]\n",
        "            if char == 'A':\n",
        "                data[i, j]= 0.1260\n",
        "            if char == 'C':\n",
        "                data[i, j]= 0.1340\n",
        "            if char == 'G':\n",
        "                data[i, j]= 0.0806\n",
        "            if char == 'T':\n",
        "                data[i, j]= 0.1335\n",
        "    return data\n",
        "\n",
        "# chatching data from excel file\n",
        "seq= np.array( read_excel('/content/filtered_data.xlsx', header= None) )\n",
        "# set the data shape\n",
        "m, n= 1156, 26592\n",
        "all_data= EIIP(seq, n)\n",
        "# create the labels: 1= Covid-19, 0= Other Human Coronavirus\n",
        "target= np.array([np.full(int(m/2), 1), np.full(int(m/2), 0)])\n",
        "target= target.reshape(m,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bring testing data out of the data**"
      ],
      "metadata": {
        "id": "8fF_G_un43Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# seed for reproducibility\n",
        "seed= 5\n",
        "random.seed(seed)\n",
        "# acquire testing data/labels from dataset\n",
        "x= random.sample(range(1156), 156)\n",
        "x= np.sort(x, axis=0)\n",
        "testing= all_data[(x)]\n",
        "testing_label= target[x]\n",
        "# remove obtained testing data from training/validation data\n",
        "data= list(all_data)\n",
        "data_label= list(target)\n",
        "for i in range(len(x)):\n",
        "  data.pop( x[ -(i+1) ] )\n",
        "  data_label.pop( x[ -(i+1) ] )\n",
        "\n",
        "data= np.array(data)"
      ],
      "metadata": {
        "id": "3pLeuoMwkV9n"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Enlarge the data by \"data slicing\"**"
      ],
      "metadata": {
        "id": "aHSkr3_n48xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data slicing function\n",
        "def sliced_array(data, data_label, K, N): # k= window size, N= number of features\n",
        "    new_data = np.zeros( (data.shape[0]*(int(N/K)), 256) )\n",
        "    new_target= [0]*( data.shape[0]*(int(N/K)) )\n",
        "    temp = []\n",
        "    p= 0\n",
        "    for i in range(data.shape[0]):\n",
        "      temp.clear()\n",
        "      for j in range(data.shape[1]):\n",
        "          temp.append( data[i,j] )\n",
        "          if ((j+1) % K) == 0:\n",
        "              new_data[p]= temp\n",
        "              new_target[p]= data_label[i][0]\n",
        "              p= p+1\n",
        "              temp.clear()\n",
        "    return new_data, new_target\n",
        "\n",
        "# call the function\n",
        "data, data_label = sliced_array(data, data_label, 256, n)\n",
        "testing, testing_label = sliced_array(testing, testing_label, 256, n)\n",
        "# check about new shapes\n",
        "print('shape of data=%s, labels size=%d' %(data.shape, len(data_label)) )\n",
        "print('shape of testing data=%s, testing labels size=%d' %(testing.shape, len(testing_label)) )"
      ],
      "metadata": {
        "id": "wuXf6M9xWrAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split data into teaining and validation data**"
      ],
      "metadata": {
        "id": "nJ7snxRS5Dej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select percentage of splitting (0.3 used)\n",
        "x_train, val_train, y_train, y_test= train_test_split(data, data_label,\n",
        "                                                      test_size=0.3, random_state=10,\n",
        "                                                      shuffle=True, stratify=data_label)\n",
        "#convert y_train from list to array\n",
        "y_train= np.array(y_train)\n",
        "y_test= np.array(y_test)\n",
        "testing_label= np.array(testing_label)"
      ],
      "metadata": {
        "id": "IWUpYlJ4mFbV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Apply FFT method with removing DC component**"
      ],
      "metadata": {
        "id": "4W9H0Mrf5MM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in [x_train, val_train, testing]:\n",
        "    # find the sample mean\n",
        "    x_mean= np.mean(i, axis=1)\n",
        "    # apply broadcasting\n",
        "    x_mean_pro= np.tile(x_mean, (256,1))\n",
        "    # sabtract sample mean from org sample to remove bias\n",
        "    x_DC_removed= i - x_mean_pro.T\n",
        "    # create FFT data\n",
        "    if   len(i) == len(x_train):\n",
        "        x_train_FFT= np.abs( fft(x_DC_removed, axis=1) )\n",
        "    elif len(i) == len(val_train):\n",
        "        val_train_FFT= np.abs( fft(x_DC_removed, axis=1))\n",
        "    elif len(i) == len(testing):\n",
        "        testing_FFT= np.abs( fft(x_DC_removed, axis=1))"
      ],
      "metadata": {
        "id": "GvrWSFnZzcIp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build the model architecture**"
      ],
      "metadata": {
        "id": "jUJsZDew5TDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# layers per block function\n",
        "def creation(layer_in, filters, kernel_size, padding):\n",
        "    layer_in= Conv1D(filters=filters, kernel_size=kernel_size, strides=1, padding=padding)(layer_in)\n",
        "    layer_in= BatchNormalization()(layer_in)\n",
        "    layer_in= Activation('relu')(layer_in)\n",
        "    layer_in= MaxPooling1D(2, strides=2)(layer_in)\n",
        "    layer_in= Dropout(0.3)(layer_in)\n",
        "    return layer_in\n",
        "# build blocks of the model\n",
        "inputs= Input(shape=( x_train.shape[1], 1))\n",
        "block_1= creation(inputs, filters=64, kernel_size=10, padding='valid')\n",
        "block_2= creation(block_1, 128, 16, 'same')\n",
        "block_3= creation(block_2, 128, 16, 'same')\n",
        "block_4= creation(block_3, 128, 16, 'same')\n",
        "block_5= creation(block_4, 64,  16, 'same')\n",
        "block_6= creation(block_5, 32,  16, 'same')\n",
        "block_7= creation(block_6, 32,  16, 'same')\n",
        "fC_layers_8= Flatten()(block_7)\n",
        "fC_layers_9= Dense(320)(fC_layers_8)\n",
        "fC_layers_10= Dropout(0.3)(fC_layers_9)\n",
        "fC_layers_11= Dense(1, activation='sigmoid')(fC_layers_10)\n",
        "# construct the model\n",
        "model= Model(inputs, fC_layers_11)\n",
        "learning_rate= 0.003\n",
        "optimizer= optimizers.Adam(learning_rate= learning_rate)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# visualize the model architecture\n",
        "model.summary()\n",
        "plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "35jXuBwFdhv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install scikeras library to create estimator**"
      ],
      "metadata": {
        "id": "Qed2_Rdl5Xb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "id": "SglK51qkHIPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Apply grid search for hyperparameters tuning**"
      ],
      "metadata": {
        "id": "eUS16kKb5isU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# call the necessary libraries\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# set the numeric data hyperparameters to select\n",
        "batch_size= [256, 512, 1024,2048, 4096]   # [256, 512, 1024] for FFT data\n",
        "epochs= [15, 20, 30, 50, 75, 100, 200, 250]  # [35, 45, 55] for FFT data\n",
        "learning_rate= [0.0005, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3] # for both\n",
        "param_grid= dict(batch_size=batch_size, epochs=epochs, learning_rate=learning_rate)\n",
        "estimator= KerasClassifier(model=model, learning_rate=learning_rate, batch_size=batch_size, epochs=epochs, verbose=0)\n",
        "optimal= GridSearchCV(estimator, param_grid=param_grid, n_jobs=None, verbose=2, cv=3)"
      ],
      "metadata": {
        "id": "e96l4e7Dr-uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit the model to find the optimal hyperparameters"
      ],
      "metadata": {
        "id": "w26tS2em5nB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_his= optimal.fit(X= x_train, y=y_train) # for FFT data, X= x_train_FFT"
      ],
      "metadata": {
        "id": "AfO2u3UawDoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate the hyperparameters scores**"
      ],
      "metadata": {
        "id": "OCx78cqI5uMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean= optimal_his.cv_results_['mean_test_score']\n",
        "time= optimal_his.cv_results_['mean_score_time']\n",
        "std= optimal_his.cv_results_['std_test_score']\n",
        "params= optimal_his.cv_results_['params']\n",
        "for a,b,c,d in zip(mean, time, std, params):\n",
        "  print('mean(%f), time(%f), std(%f), parameters(%s)'%(a,b,c,d))\n",
        "print('best score is %f with parameters %s'%(optimal_his.best_score_, optimal_his.best_params_))"
      ],
      "metadata": {
        "id": "Lc8Kgt9o-inr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Finally, fit the model with selected hyperparameters**\n"
      ],
      "metadata": {
        "id": "xcMJo7Nh5yKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy the model to fit it with two different data\n",
        "M= clone_model(model)\n",
        "optimizer= optimizers.Adam(learning_rate= 0.003)\n",
        "M.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# fit the numeric data\n",
        "his= M.fit(x_train, y_train, batch_size=256, epochs=200, verbose=1,\n",
        "         validation_data=[val_train, y_test], workers=0) # for FFT data: batch_size=512, epochs=55"
      ],
      "metadata": {
        "id": "UR9LqQDL-yPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plot loss-accuracy curve**"
      ],
      "metadata": {
        "id": "0Z3U4exA53_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tr_acc = his.history['accuracy']\n",
        "tr_loss = his.history['loss']\n",
        "val_acc = his.history['val_accuracy']\n",
        "val_loss = his.history['val_loss']\n",
        "Epochs = [i+1 for i in range( len(tr_acc) )]\n",
        "# Plot training history\n",
        "plt.figure(figsize= (20, 8))\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\n",
        "plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\n",
        "plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.tight_layout\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kchoeE92eFGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Score the model on validation data**"
      ],
      "metadata": {
        "id": "J0ynuT4L6Dnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score= M.evaluate(val_train, y_test, verbose=1)\n",
        "print('score of numeric data:\\n loss=%f, accuracy=%f' %(score[0], score[1]) )"
      ],
      "metadata": {
        "id": "ugYLvREfeRqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Predict testing data**"
      ],
      "metadata": {
        "id": "sK-dprHy6Hxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction= M.predict(testing, batch_size=256 ) # for FFT data: testing_FFT, batch_size=512\n",
        "# convert the results from float to int\n",
        "prediction= np.int_(prediction)"
      ],
      "metadata": {
        "id": "mbHUAaK8eU23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Show the classification report and confusion matrix**\n"
      ],
      "metadata": {
        "id": "B_MU4LeR6Lx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf= classification_report(testing_label, prediction)\n",
        "cm= confusion_matrix(testing_label, prediction)\n",
        "[tp, fp], [fn, tn]= confusion_matrix(testing_label, prediction)\n",
        "print(cm)\n",
        "print('tp=%d\\nfp=%d\\nfn=%d\\ntn=%d' %(tp, fp, fn, tn))\n",
        "print(clf)"
      ],
      "metadata": {
        "id": "sLnZ1yzveXBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Display the confusion matrix**"
      ],
      "metadata": {
        "id": "qwfXhqJ36PsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "disp= ConfusionMatrixDisplay(cm, display_labels=['Covid19','Other Coronavirus'])\n",
        "disp.plot(cmap='gray')"
      ],
      "metadata": {
        "id": "DSiHmy4eejpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Determine all metrics**"
      ],
      "metadata": {
        "id": "BgYSTw9-6U00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('accuracy=', ( accuracy_score(testing_label, prediction)*100) )\n",
        "print('specificity=', ( tn/(tn+fp) )*100  )\n",
        "sensitivity= ( tp/(tp+fn) )*100\n",
        "print('sensitivity=', sensitivity)\n",
        "precision= precision_score(testing_label, prediction) *100\n",
        "print('precision=', precision)\n",
        "F_score= (2*precision*sensitivity) / (precision+sensitivity)\n",
        "print('F-score=', F_score)"
      ],
      "metadata": {
        "id": "v3NfqQPBfT55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plot ROC curve and AUC score**"
      ],
      "metadata": {
        "id": "mUkFoGa06dCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, threshold= roc_curve(testing_label, prediction)\n",
        "auc= roc_auc_score(testing_label, prediction)\n",
        "plt.style.use('default')\n",
        "plt.plot(fpr,tpr, label= 'FFT & Numeric AUC='+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.xlim([-0.05,1])\n",
        "plt.ylim([0,1.04])\n",
        "plt.grid()\n",
        "plt.xlabel('1-specificity( fpr)')\n",
        "plt.ylabel('sensitivity (tpr)')\n",
        "plt.title('ROC')"
      ],
      "metadata": {
        "id": "VxgMIzzYeoK5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}